{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing dependencies and setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# + pyarrow (fast CSV/Parquet IO) + polars (fast ETL) + fastparquet (optional)\n",
        "!pip install pandas numpy tqdm scikit-learn xgboost lightgbm matplotlib pyarrow polars fastparquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# Prep: make your combined file compatible with the existing splitter.\n",
        "import os, shutil, uuid\n",
        "import polars as pl\n",
        "\n",
        "folder = os.getcwd()\n",
        "combined = os.path.join(folder, \"combined_2.parquet\")   # your single merged file\n",
        "alias    = os.path.join(folder, \"single_clean.parquet\")       # matches \"*_clean.parquet\" glob\n",
        "\n",
        "if not os.path.exists(combined):\n",
        "    raise FileNotFoundError(f\"Missing: {combined}\")\n",
        "\n",
        "# 1) Strip pre-existing 'rid' so the splitter can add its own\n",
        "schema = pl.scan_parquet(combined).schema\n",
        "if \"rid\" in schema:\n",
        "    tmp_path = combined + f\".tmp.{uuid.uuid4().hex}.parquet\"\n",
        "    print(f\"→ 'rid' found in {combined}. Rewriting without it → {tmp_path}\")\n",
        "    (\n",
        "        pl.scan_parquet(combined)\n",
        "          .select(pl.all().exclude(\"rid\"))\n",
        "          .sink_parquet(\n",
        "              tmp_path,\n",
        "              compression=\"zstd\",\n",
        "              compression_level=1,\n",
        "              statistics=False,\n",
        "              maintain_order=False,\n",
        "          )\n",
        "    )\n",
        "    shutil.move(tmp_path, combined)\n",
        "    print(\"✓ 'rid' removed.\")\n",
        "else:\n",
        "    print(\"✓ No 'rid' in combined file (nothing to remove).\")\n",
        "\n",
        "# 2) Ensure a *_clean.parquet exists (so your splitter's glob works unchanged)\n",
        "if os.path.exists(alias):\n",
        "    print(f\"✓ Alias already exists: {alias}\")\n",
        "else:\n",
        "    try:\n",
        "        os.symlink(os.path.basename(combined), alias)\n",
        "        print(f\"→ Created symlink: {alias} → {combined}\")\n",
        "    except Exception:\n",
        "        # Fallback if symlink not allowed (e.g., Windows without perms)\n",
        "        shutil.copyfile(combined, alias)\n",
        "        print(f\"→ Symlink not allowed; copied file: {alias}\")\n",
        "\n",
        "print(\"\\nAll set. Run your original split script as-is.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "folder = os.getcwd()\n",
        "pattern = os.path.join(folder, \"*_clean.parquet\")\n",
        "\n",
        "# 1) Lazily scan all parts (single call; supports glob)\n",
        "lazy_all = pl.scan_parquet(pattern)\n",
        "\n",
        "# 2) Add a stable row id (after concat) and ensure Label dtype\n",
        "#    We will ONLY collect (rid, Label), not the whole table.\n",
        "lazy_all = lazy_all.with_row_index(\"rid\").with_columns(pl.col(\"Label\").cast(pl.Utf8))\n",
        "\n",
        "# 3) Collect tiny metadata needed for splitting\n",
        "meta = lazy_all.select(\"rid\", \"Label\").collect(engine=\"streaming\")\n",
        "\n",
        "if \"Label\" not in meta.columns:\n",
        "    raise RuntimeError(\"Expected a 'Label' column in the dataset.\")\n",
        "\n",
        "# 4) Decide if we can stratify (≥2 classes and each has ≥2 rows)\n",
        "use_stratify = False\n",
        "counts = meta.group_by(\"Label\").len().select(\"len\").to_series().to_numpy()\n",
        "if meta[\"Label\"].n_unique() >= 2 and (counts >= 2).all():\n",
        "    use_stratify = True\n",
        "\n",
        "# 5) Split on *indices* (NumPy) — tiny and fast\n",
        "rid = meta[\"rid\"].to_numpy()\n",
        "labels = meta[\"Label\"].to_numpy()\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    rid,\n",
        "    test_size=0.20,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=labels if use_stratify else None,\n",
        ")\n",
        "\n",
        "# 6) Build small row-id tables and join (semi) to stream rows to Parquet\n",
        "train_ids = pl.DataFrame({\"rid\": train_idx}).lazy()\n",
        "test_ids  = pl.DataFrame({\"rid\": test_idx}).lazy()\n",
        "\n",
        "lazy_with_id = lazy_all  # already has \"rid\" column\n",
        "\n",
        "lazy_train = lazy_with_id.join(train_ids, on=\"rid\", how=\"semi\")\n",
        "lazy_test  = lazy_with_id.join(test_ids,  on=\"rid\", how=\"semi\")\n",
        "\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "test_parquet  = os.path.join(folder, \"test.parquet\")\n",
        "\n",
        "# 7) Stream directly to Parquet (fast path; no full materialization)\n",
        "#    Use a fast codec/level for speed; tweak as you prefer.\n",
        "lazy_train.sink_parquet(train_parquet, compression=\"zstd\", compression_level=1, statistics=False)\n",
        "lazy_test.sink_parquet(test_parquet,   compression=\"zstd\", compression_level=1, statistics=False)\n",
        "\n",
        "print(\n",
        "    \"Data split complete:\\n\"\n",
        "    f\" • Pattern: {pattern}\\n\"\n",
        "    f\" • Training rows: {len(train_idx)}\\n\"\n",
        "    f\" • Test rows: {len(test_idx)}\\n\"\n",
        "    f\" • Stratified: {use_stratify}\\n\"\n",
        "    f\" • Saved: {train_parquet}, {test_parquet}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import polars as pl\n",
        "\n",
        "folder = os.getcwd()\n",
        "pattern = os.path.join(folder, \"*_clean.parquet\")\n",
        "\n",
        "# Scan, then select only the Label column (projection pushdown will keep it fast).\n",
        "q = (\n",
        "    pl.scan_parquet(pattern)\n",
        "      .select(pl.col(\"Label\").cast(pl.Categorical))\n",
        "      .filter(pl.col(\"Label\").is_not_null())\n",
        ")\n",
        "\n",
        "# Total rows (collect just a tiny scalar)\n",
        "n_rows = (\n",
        "    q.select(pl.len().alias(\"rows\"))\n",
        "     .collect(engine=\"streaming\")[\"rows\"][0]\n",
        ")\n",
        "\n",
        "# Label counts (materialize only the small result)\n",
        "label_counts = (\n",
        "    q.group_by(\"Label\")\n",
        "     .len()\n",
        "     .sort(\"len\", descending=True)\n",
        "     .collect(engine=\"streaming\")\n",
        "     .rename({\"len\": \"count\"})\n",
        ")\n",
        "\n",
        "print(\"Analyzing original Parquet files (glob): *_clean.parquet\")\n",
        "print(f\"Total rows: {n_rows}\\n\")\n",
        "print(\"Unique labels and their counts:\")\n",
        "for label, count in zip(label_counts[\"Label\"].to_list(),\n",
        "                        label_counts[\"count\"].to_list()):\n",
        "    print(f\"  {label}: {count}\")\n",
        "print(f\"\\nTotal unique labels: {label_counts.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We deduce that there are 20(19+1) Classes (Attack Labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "folder = os.getcwd()\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "\n",
        "# 1) Load train\n",
        "train_pl = pl.read_parquet(train_parquet)\n",
        "y_column = \"Label\"\n",
        "if y_column not in train_pl.columns:\n",
        "    raise RuntimeError(\"Expected 'Label' in train.parquet\")\n",
        "\n",
        "# 2) Numeric cols (exclude label)\n",
        "numeric_cols = [c for c in train_pl.select(cs.numeric()).columns if c != y_column]\n",
        "if not numeric_cols:\n",
        "    raise RuntimeError(\"No numeric feature columns found.\")\n",
        "\n",
        "# 3) Medians on TRAIN ONLY\n",
        "meds_s = train_pl.select([pl.col(c).median().alias(c) for c in numeric_cols])\n",
        "medians = {c: float(meds_s[c][0]) for c in numeric_cols}\n",
        "\n",
        "# 4) Impute to float32 → view as float64 for stats\n",
        "def to_imputed_f32(df: pl.DataFrame, cols: list[str], med: dict[str, float]) -> np.ndarray:\n",
        "    df = df.with_columns([pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in cols])\n",
        "    df = df.with_columns([pl.col(c).fill_null(med[c]).fill_nan(med[c]).cast(pl.Float32).alias(c) for c in cols])\n",
        "    return df.select(cols).to_numpy().astype(np.float32, copy=False)\n",
        "\n",
        "X_train_for_scaler = to_imputed_f32(train_pl, numeric_cols, medians).astype(np.float64, copy=False)\n",
        "\n",
        "# 4.1) Drop zero/near-zero variance cols once\n",
        "stds = X_train_for_scaler.std(axis=0, dtype=np.float64)  # ddof=0 like StandardScaler\n",
        "EPS_STD = 1e-12\n",
        "keep_mask = stds > EPS_STD\n",
        "if not keep_mask.all():\n",
        "    dropped = [c for c, k in zip(numeric_cols, keep_mask) if not k]\n",
        "    print(f\"Dropping {len(dropped)} ~zero-variance cols:\", dropped)\n",
        "    numeric_cols = [c for c, k in zip(numeric_cols, keep_mask) if k]\n",
        "    X_train_for_scaler = X_train_for_scaler[:, keep_mask]\n",
        "    medians = {c: medians[c] for c in numeric_cols}\n",
        "\n",
        "# 5) Fit scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_for_scaler)\n",
        "\n",
        "# 5.1) Save LR keep indices for near-constant scales\n",
        "EPS_SCALE = 1e-6\n",
        "lr_keep_mask = scaler.scale_ >= EPS_SCALE\n",
        "lr_keep_idx = np.where(lr_keep_mask)[0].astype(np.int32)\n",
        "if not lr_keep_mask.all():\n",
        "    dropped_lr = [c for c, k in zip(numeric_cols, lr_keep_mask) if not k]\n",
        "    print(f\"[LR path] Dropping {len(dropped_lr)} near-constant cols by scale:\", dropped_lr)\n",
        "\n",
        "# 6) Save artifacts\n",
        "artifacts = {\n",
        "    \"numeric_cols\": numeric_cols,\n",
        "    \"medians\": medians,\n",
        "    \"scaler_mean\": scaler.mean_.astype(np.float64),\n",
        "    \"scaler_scale\": scaler.scale_.astype(np.float64),  # keep raw\n",
        "    \"lr_keep_idx\": lr_keep_idx,\n",
        "}\n",
        "with open(os.path.join(folder, \"preproc.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(artifacts, f)\n",
        "\n",
        "print(f\"Saved preproc.pkl with {len(numeric_cols)} cols (LR cols kept: {lr_keep_idx.size}).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoJkN86QQLNX"
      },
      "source": [
        "## 20(19+1) Classes (Attack Labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBSTBnT1rUSa",
        "outputId": "e179b652-e24d-4c73-eaee-e6829ddc5253"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# --- Threading: let libs use all cores (don't cap OMP to 1)\n",
        "import os, time, warnings\n",
        "CORES = os.cpu_count() or 4\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(CORES)        # XGBoost / OpenMP users\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(CORES)        # BLAS for LR/NumPy\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(CORES)\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(min(CORES, 8))\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --------------------------\n",
        "# 1) Load train/test Parquet (Polars)\n",
        "# --------------------------\n",
        "folder = os.getcwd()\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "test_parquet  = os.path.join(folder, \"test.parquet\")\n",
        "if not (os.path.exists(train_parquet) and os.path.exists(test_parquet)):\n",
        "    raise RuntimeError(\"Missing train.parquet/test.parquet. Run the splitter step first.\")\n",
        "\n",
        "train_pl = pl.read_parquet(train_parquet)\n",
        "test_pl  = pl.read_parquet(test_parquet)\n",
        "\n",
        "# --------------------------\n",
        "# 2) Columns: numeric features + label\n",
        "# --------------------------\n",
        "y_column = \"Label\"\n",
        "if y_column not in train_pl.columns or y_column not in test_pl.columns:\n",
        "    raise RuntimeError(\"Expected a 'Label' column in both train and test dataframes.\")\n",
        "\n",
        "schema = train_pl.schema\n",
        "numeric_cols = [c for c, dt in schema.items() if c != y_column and dt in pl.NUMERIC_DTYPES]\n",
        "if not numeric_cols:\n",
        "    raise RuntimeError(\"No numeric feature columns found.\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) Feature hygiene in Polars (inf -> null; fill null/nan with median; cast to f32)\n",
        "# --------------------------\n",
        "def prep_numeric(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    # replace ±inf with null\n",
        "    df = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    # compute medians once\n",
        "    meds = df.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    # fill nulls/NaNs with medians and cast to float32\n",
        "    df = df.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df\n",
        "\n",
        "train_pl = prep_numeric(train_pl, numeric_cols)\n",
        "test_pl  = prep_numeric(test_pl,  numeric_cols)\n",
        "\n",
        "# --------------------------\n",
        "# 4) To NumPy (float32, optionally Fortran order)\n",
        "# --------------------------\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full = to_f32_f_order(train_pl, numeric_cols)\n",
        "X_test       = to_f32_f_order(test_pl,  numeric_cols)\n",
        "\n",
        "# Labels\n",
        "y_train_raw = train_pl.select(y_column).to_series().to_numpy()\n",
        "y_test_raw  = test_pl.select(y_column).to_series().to_numpy()\n",
        "\n",
        "# --------------------------\n",
        "# 5) Label encoding on union(train,test) to avoid unseen-class errors\n",
        "# --------------------------\n",
        "le = LabelEncoder()\n",
        "le.fit(np.concatenate([y_train_raw, y_test_raw]).astype(str))\n",
        "y_train_full = le.transform(y_train_raw.astype(str))\n",
        "y_test       = le.transform(y_test_raw.astype(str))\n",
        "\n",
        "# --------------------------\n",
        "# 6) Scaling for LR in float32 (apply StandardScaler params in f32)\n",
        "# --------------------------\n",
        "scaler_path = os.path.join(folder, \"preproc.pkl\")\n",
        "if not os.path.exists(scaler_path):\n",
        "    raise RuntimeError(\"preproc.pkl not found. Run the preprocessing step first.\")\n",
        "with open(scaler_path, \"rb\") as f:\n",
        "    artifacts = pickle.load(f)\n",
        "\n",
        "mean_f32  = np.asarray(artifacts[\"scaler_mean\"],  dtype=np.float32)\n",
        "scale_f32 = np.asarray(artifacts[\"scaler_scale\"], dtype=np.float32)\n",
        "\n",
        "if mean_f32 is None or scale_f32 is None:\n",
        "    raise RuntimeError(\"Loaded scaler lacks mean_/scale_. Refit with StandardScaler.\")\n",
        "scale_safe = np.where(scale_f32 == 0.0, 1.0, scale_f32)\n",
        "\n",
        "def apply_scaler_f32(X_f):\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full = apply_scaler_f32(X_train_full)\n",
        "X_test_lr       = apply_scaler_f32(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# 7) Validation split for early stopping\n",
        "# --------------------------\n",
        "X_tr_num, X_val_num, y_tr, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
        ")\n",
        "X_tr_lr,  X_val_lr  = train_test_split(\n",
        "    X_train_lr_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 8) Models (light defaults) + boosters with early stopping\n",
        "# --------------------------\n",
        "RANDOM_STATE = 42\n",
        "ML_models = [\n",
        "    (\"LogisticRegression\", LogisticRegression(\n",
        "        solver=\"lbfgs\", penalty=\"l2\", max_iter=1000, tol=2e-3, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"DecisionTreeClassifier\", DecisionTreeClassifier(\n",
        "        criterion=\"entropy\", max_depth=5, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"RandomForestClassifier\", RandomForestClassifier(\n",
        "        n_estimators=100, max_features=\"sqrt\", n_jobs=-1, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"AdaBoostClassifier\", AdaBoostClassifier(\n",
        "        n_estimators=50, learning_rate=0.5, random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    (\"BaggingClassifier\", BaggingClassifier(\n",
        "        n_estimators=10, n_jobs=-1, random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    (\"LGBMClassifier\", LGBMClassifier(\n",
        "        n_estimators=4000, num_leaves=48, learning_rate=0.06,\n",
        "        subsample=0.8, colsample_bytree=0.8,\n",
        "        n_jobs=CORES, random_state=RANDOM_STATE\n",
        "    )),\n",
        "]\n",
        "\n",
        "# --------------------------\n",
        "# 9) Report\n",
        "# --------------------------\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "open(report_path, \"w\").close()\n",
        "\n",
        "def log_metrics(name, model, X_te, y_te):\n",
        "    y_pred = model.predict(X_te)\n",
        "    acc  = accuracy_score(y_te, y_pred)\n",
        "    rec  = recall_score(y_te, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_te, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_te, y_pred, average=\"macro\")\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 10) Train / time / save / evaluate\n",
        "# --------------------------\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models\"):\n",
        "    print(f\"\\n▶ Training {name}\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr, y_tr)\n",
        "        Xte = X_test_lr\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        Xte = X_test  # test features for XGB always unscaled numeric\n",
        "\n",
        "    # Try new API (callbacks)\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback  # may not exist on very old versions\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num, y_tr,\n",
        "                eval_set=[(X_val_num, y_val)],\n",
        "                callbacks=es,\n",
        "        )\n",
        "        except (TypeError, ImportError):\n",
        "            # Fallback: older API with early_stopping_rounds in fit\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num, y_tr,\n",
        "                    eval_set=[(X_val_num, y_val)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Last resort: no early stopping\n",
        "                model.fit(X_tr_num, y_tr)   \n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num, y_tr,\n",
        "            eval_set=[(X_val_num, y_val)],\n",
        "            eval_metric=\"multi_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num, y_tr)\n",
        "        Xte = X_test\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save\n",
        "    with open(os.path.join(folder, f\"{name}_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    print(f\"▶ Testing {name}\")\n",
        "    log_metrics(name, model, Xte, y_test)\n",
        "\n",
        "print(f\"\\nAll models trained, tested, and metrics logged to '{report_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for any saved model\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match the filename you saved) ---\n",
        "model_name = \"LGBMClassifier\"        # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "model_path = os.path.join(folder, f\"{model_name}_model.pkl\")\n",
        "\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features\n",
        "Xte = X_test_lr if model_name == \"LogisticRegression\" else X_test\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels:\n",
        "# - y_test is already integer-encoded with LabelEncoder\n",
        "# - Use index range for confusion_matrix, and le.classes_ as display labels\n",
        "idx_labels   = np.arange(len(le.classes_))  # 0..K-1\n",
        "display_lbls = le.classes_\n",
        "\n",
        "# Raw counts CM\n",
        "cm = confusion_matrix(y_test, y_pred, labels=idx_labels)\n",
        "\n",
        "# Save counts CM to CSV\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "# Plot counts CM\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name}\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}.png\"), bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "# Row-normalized (per true class)\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # avoid div/0 if a class is absent\n",
        "    cm_norm = (cm / row_sums)\n",
        "\n",
        "# Save normalized CM to CSV\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "# Plot normalized CM\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name}\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}.png\"), bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import polars as pl\n",
        "\n",
        "# Combine train/test labels\n",
        "df = pl.concat([train_pl.select(\"Label\"), test_pl.select(\"Label\")])\n",
        "\n",
        "# Count labels\n",
        "label_counts = (\n",
        "    df.group_by(\"Label\")\n",
        "      .len()\n",
        "      .sort(\"len\", descending=True)\n",
        ")\n",
        "\n",
        "# Save to TXT\n",
        "out_path = os.path.join(folder, \"label_counts.txt\")\n",
        "with open(out_path, \"w\") as f:\n",
        "    for row in label_counts.iter_rows():\n",
        "        f.write(f\"{row[0]}: {row[1]}\\n\")\n",
        "\n",
        "print(f\"✅ Saved labels and counts to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7(6+1) Classes (Attack Labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We group the attacks in classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XbS_625DT9ui"
      },
      "outputs": [],
      "source": [
        "dict_7classes = {}\n",
        "\n",
        "# --- DDoS ---\n",
        "dict_7classes['DDoS_ICMP'] = 'DDoS'\n",
        "dict_7classes['DDoS_UDP'] = 'DDoS'\n",
        "dict_7classes['DDoS_TCP'] = 'DDoS'\n",
        "dict_7classes['DDoS_SYN'] = 'DDoS'\n",
        "\n",
        "# --- DoS ---\n",
        "dict_7classes['DoS_UDP'] = 'DoS'\n",
        "dict_7classes['DoS_TCP'] = 'DoS'\n",
        "dict_7classes['DoS_SYN'] = 'DoS'\n",
        "dict_7classes['DoS_ICMP'] = 'DoS'\n",
        "\n",
        "# --- Benign ---\n",
        "dict_7classes['Benign'] = 'Benign'\n",
        "\n",
        "# --- Spoofing ---\n",
        "dict_7classes['Spoofing_ARP'] = 'Spoofing'\n",
        "\n",
        "# --- Recon ---\n",
        "dict_7classes['Recon_Ping_Sweep'] = 'Recon'\n",
        "dict_7classes['Recon_OS_Scan'] = 'Recon'\n",
        "dict_7classes['Recon_Port_Scan'] = 'Recon'\n",
        "dict_7classes['Recon_Vulnerability_Scan'] = 'Recon'\n",
        "\n",
        "# --- MQTT ---\n",
        "dict_7classes['MQTT_DDoS_Flooding'] = 'MQTT'\n",
        "dict_7classes['MQTT_DoS_Flooding'] = 'MQTT'\n",
        "dict_7classes['MQTT_Malformed_Data'] = 'MQTT'\n",
        "dict_7classes['MQTT_DoS_Attack'] = 'MQTT'\n",
        "dict_7classes['MQTT_SlowITe'] = 'MQTT'\n",
        "\n",
        "dict_7classes['Bruteforce'] = 'BruteForce'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 7-class grouping run (Polars → NumPy) ===\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import os, pickle, time\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "y_column = \"Label\"\n",
        "\n",
        "# --- Map labels to 7 classes (drop anything unmapped) ---\n",
        "def map_labels_pl(df: pl.DataFrame, mapping: dict) -> pl.DataFrame:\n",
        "    mapped = df.with_columns(\n",
        "        pl.col(y_column).cast(pl.Utf8).replace(mapping, default=None).alias(y_column)\n",
        "    )\n",
        "    # Drop rows where mapping failed\n",
        "    return mapped.drop_nulls(subset=[y_column])\n",
        "\n",
        "train_pl_7 = map_labels_pl(train_pl, dict_7classes)\n",
        "test_pl_7  = map_labels_pl(test_pl,  dict_7classes)\n",
        "\n",
        "# --- Hygiene per split: replace ±inf→null, fill with medians, cast float32 ---\n",
        "def prep_numeric_pl(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_pl_7 = prep_numeric_pl(train_pl_7, numeric_cols)\n",
        "test_pl_7  = prep_numeric_pl(test_pl_7,  numeric_cols)\n",
        "\n",
        "# --- To NumPy (float32, Fortran order) ---\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full_7 = to_f32_f_order(train_pl_7, numeric_cols)\n",
        "X_test_7       = to_f32_f_order(test_pl_7,  numeric_cols)\n",
        "\n",
        "# --- Labels (string → LabelEncoder over union) ---\n",
        "y_train_7_raw = train_pl_7.select(y_column).to_series().to_numpy().astype(str)\n",
        "y_test_7_raw  = test_pl_7.select(y_column).to_series().to_numpy().astype(str)\n",
        "\n",
        "le7 = LabelEncoder()\n",
        "le7.fit(np.concatenate([y_train_7_raw, y_test_7_raw]))\n",
        "y_train_full_7 = le7.transform(y_train_7_raw)\n",
        "y_test_7       = le7.transform(y_test_7_raw)\n",
        "\n",
        "# --- Scaling for LR (reuse your pre-fitted scaler params already loaded) ---\n",
        "def apply_scaler_f32(X_f):\n",
        "    # reusing mean_f32 and scale_safe defined earlier in your session\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full_7 = apply_scaler_f32(X_train_full_7)\n",
        "X_test_lr_7       = apply_scaler_f32(X_test_7)\n",
        "\n",
        "# --- Train/val split (stratified) for early stopping boosters ---\n",
        "X_tr_num7, X_val_num7, y_tr7, y_val7 = train_test_split(\n",
        "    X_train_full_7, y_train_full_7, test_size=0.15, random_state=42, stratify=y_train_full_7\n",
        ")\n",
        "X_tr_lr7,  X_val_lr7  = train_test_split(\n",
        "    X_train_lr_full_7, test_size=0.15, random_state=42, stratify=y_train_full_7\n",
        ")\n",
        "\n",
        "# --- Append header to the same report.txt ---\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "with open(report_path, \"a\") as fp:\n",
        "    fp.write(\"\\n\\n===== Grouped 7-Class Run =====\\n\")\n",
        "    fp.write(\"Classes (encoded order): \" + \", \".join(map(str, le7.classes_)) + \"\\n\")\n",
        "\n",
        "# --- Train / evaluate each model on 7 classes ---\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models (7-class)\"):\n",
        "    print(f\"\\n▶ Training {name} (7-class)\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr7, y_tr7)\n",
        "        Xte = X_test_lr_7\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        # (Only if you have XGB in ML_models; your current list does not include it.)\n",
        "        Xte = X_test_7\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num7, y_tr7,\n",
        "                eval_set=[(X_val_num7, y_val7)],\n",
        "                callbacks=es,\n",
        "            )\n",
        "        except (TypeError, ImportError):\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num7, y_tr7,\n",
        "                    eval_set=[(X_val_num7, y_val7)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                model.fit(X_tr_num7, y_tr7)\n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num7, y_tr7,\n",
        "            eval_set=[(X_val_num7, y_val7)],\n",
        "            eval_metric=\"multi_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test_7\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num7, y_tr7)\n",
        "        Xte = X_test_7\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} (7-class) trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save model\n",
        "    with open(os.path.join(folder, f\"{name}_7classes_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"▶ Testing {name} (7-class)\")\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc  = accuracy_score(y_test_7, y_pred)\n",
        "    rec  = recall_score(y_test_7, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_test_7, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_test_7, y_pred, average=\"macro\")\n",
        "\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} (7 Classes) #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "print(\"\\n✅ All 7-class models trained & logged to 'report.txt'.\")\n",
        "print(\"Encoded class order (7-class):\", list(le7.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for 7-class models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match what you saved with _7classes) ---\n",
        "model_name = \"LGBMClassifier\"        # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "suffix = \"7classes\"                   # keeps filenames distinct\n",
        "\n",
        "model_path = os.path.join(folder, f\"{model_name}_{suffix}_model.pkl\")\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features & labels for 7-class run\n",
        "Xte = X_test_lr_7 if model_name == \"LogisticRegression\" else X_test_7\n",
        "y_true = y_test_7\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels/ticks: use full index range to force consistent matrix shape, display names from le7\n",
        "idx_labels   = np.arange(len(le7.classes_))   # 0..K-1\n",
        "display_lbls = le7.classes_\n",
        "\n",
        "# --- Raw counts CM ---\n",
        "cm = confusion_matrix(y_true, y_pred, labels=idx_labels)\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "# --- Row-normalized CM (per true class) ---\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # avoid div/0 if a class is absent\n",
        "    cm_norm = cm / row_sums\n",
        "\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}_{suffix}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}_{suffix}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2(1+1) Classes (Attack or Benign)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGN2itN2ZpgY"
      },
      "source": [
        "Now, let's move to just 2 classifiers(attack, benign)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bQAp_f24ZuKI"
      },
      "outputs": [],
      "source": [
        "dict_2classes = {}\n",
        "\n",
        "# --- Benign ---\n",
        "dict_2classes['Benign'] = 'Benign'\n",
        "\n",
        "# --- DDoS ---\n",
        "dict_2classes['DDoS_ICMP'] = 'Attack'\n",
        "dict_2classes['DDoS_UDP'] = 'Attack'\n",
        "dict_2classes['DDoS_TCP'] = 'Attack'\n",
        "dict_2classes['DDoS_SYN'] = 'Attack'\n",
        "\n",
        "# --- DoS ---\n",
        "dict_2classes['DoS_UDP'] = 'Attack'\n",
        "dict_2classes['DoS_TCP'] = 'Attack'\n",
        "dict_2classes['DoS_SYN'] = 'Attack'\n",
        "dict_2classes['DoS_ICMP'] = 'Attack'\n",
        "\n",
        "# --- Spoofing ---\n",
        "dict_2classes['Spoofing_ARP'] = 'Attack'\n",
        "\n",
        "# --- Recon ---\n",
        "dict_2classes['Recon_Ping_Sweep'] = 'Attack'\n",
        "dict_2classes['Recon_OS_Scan'] = 'Attack'\n",
        "dict_2classes['Recon_Port_Scan'] = 'Attack'\n",
        "dict_2classes['Recon_Vulnerability_Scan'] = 'Attack'\n",
        "\n",
        "# --- BruteForce ---\n",
        "dict_2classes['Bruteforce'] = 'Attack'\n",
        "\n",
        "# --- MQTT ---\n",
        "dict_2classes['MQTT_DDoS_Flooding'] = 'Attack'\n",
        "dict_2classes['MQTT_DoS_Flooding'] = 'Attack'\n",
        "dict_2classes['MQTT_DoS_Attack'] = 'Attack'\n",
        "dict_2classes['MQTT_Malformed_Data'] = 'Attack'\n",
        "dict_2classes['MQTT_SlowITe'] = 'Attack'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2-class grouping run (Polars → NumPy) ===\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import os, pickle, time\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_column = \"Label\"\n",
        "\n",
        "# --- Map labels to 2 classes (drop anything unmapped) ---\n",
        "def map_labels_pl(df: pl.DataFrame, mapping: dict) -> pl.DataFrame:\n",
        "    mapped = df.with_columns(\n",
        "        pl.col(y_column).cast(pl.Utf8).replace(mapping, default=None).alias(y_column)\n",
        "    )\n",
        "    return mapped.drop_nulls(subset=[y_column])\n",
        "\n",
        "train_pl_2 = map_labels_pl(train_pl, dict_2classes)\n",
        "test_pl_2  = map_labels_pl(test_pl,  dict_2classes)\n",
        "\n",
        "# --- Hygiene per split: ±inf→null, fill medians, cast float32 ---\n",
        "def prep_numeric_pl(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_pl_2 = prep_numeric_pl(train_pl_2, numeric_cols)\n",
        "test_pl_2  = prep_numeric_pl(test_pl_2,  numeric_cols)\n",
        "\n",
        "# --- To NumPy (float32, Fortran order) ---\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full_2 = to_f32_f_order(train_pl_2, numeric_cols)\n",
        "X_test_2       = to_f32_f_order(test_pl_2,  numeric_cols)\n",
        "\n",
        "# --- Labels (string → LabelEncoder over union) ---\n",
        "y_train_2_raw = train_pl_2.select(y_column).to_series().to_numpy().astype(str)\n",
        "y_test_2_raw  = test_pl_2.select(y_column).to_series().to_numpy().astype(str)\n",
        "\n",
        "le2 = LabelEncoder()\n",
        "le2.fit(np.concatenate([y_train_2_raw, y_test_2_raw]))\n",
        "y_train_full_2 = le2.transform(y_train_2_raw)\n",
        "y_test_2       = le2.transform(y_test_2_raw)\n",
        "\n",
        "# --- Scaling for LR (reuse your pre-fitted scaler params already loaded) ---\n",
        "def apply_scaler_f32(X_f):\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full_2 = apply_scaler_f32(X_train_full_2)\n",
        "X_test_lr_2       = apply_scaler_f32(X_test_2)\n",
        "\n",
        "# --- Train/val split (stratified) for early stopping boosters ---\n",
        "X_tr_num2, X_val_num2, y_tr2, y_val2 = train_test_split(\n",
        "    X_train_full_2, y_train_full_2, test_size=0.15, random_state=42, stratify=y_train_full_2\n",
        ")\n",
        "X_tr_lr2,  X_val_lr2  = train_test_split(\n",
        "    X_train_lr_full_2, test_size=0.15, random_state=42, stratify=y_train_full_2\n",
        ")\n",
        "\n",
        "# --- Append header to the same report.txt ---\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "with open(report_path, \"a\") as fp:\n",
        "    fp.write(\"\\n\\n===== Grouped 2-Class Run =====\\n\")\n",
        "    fp.write(\"Classes (encoded order): \" + \", \".join(map(str, le2.classes_)) + \"\\n\")\n",
        "\n",
        "# --- Train / evaluate each model on 2 classes ---\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models (2-class)\"):\n",
        "    print(f\"\\n▶ Training {name} (2-class)\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr2, y_tr2)\n",
        "        Xte = X_test_lr_2\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        # Only if XGB is in your ML_models\n",
        "        Xte = X_test_2\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num2, y_tr2,\n",
        "                eval_set=[(X_val_num2, y_val2)],\n",
        "                callbacks=es,\n",
        "            )\n",
        "        except (TypeError, ImportError):\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num2, y_tr2,\n",
        "                    eval_set=[(X_val_num2, y_val2)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                model.fit(X_tr_num2, y_tr2)\n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num2, y_tr2,\n",
        "            eval_set=[(X_val_num2, y_val2)],\n",
        "            eval_metric=\"binary_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test_2\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num2, y_tr2)\n",
        "        Xte = X_test_2\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} (2-class) trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save model\n",
        "    with open(os.path.join(folder, f\"{name}_2classes_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"▶ Testing {name} (2-class)\")\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc  = accuracy_score(y_test_2, y_pred)\n",
        "    rec  = recall_score(y_test_2, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_test_2, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_test_2, y_pred, average=\"macro\")\n",
        "\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} (2 Classes) #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "print(\"\\n✅ All 2-class models trained & logged to 'report.txt'.\")\n",
        "print(\"Encoded class order (2-class):\", list(le2.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for 2-class models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match what you saved with _2classes) ---\n",
        "model_name = \"LGBMClassifier\"   # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "suffix = \"2classes\"             # keeps filenames distinct\n",
        "\n",
        "model_path = os.path.join(folder, f\"{model_name}_{suffix}_model.pkl\")\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features & labels for 2-class run\n",
        "Xte    = X_test_lr_2 if model_name == \"LogisticRegression\" else X_test_2\n",
        "y_true = y_test_2\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels/ticks: use full index range to force consistent matrix shape; display names from le2\n",
        "idx_labels   = np.arange(len(le2.classes_))   # 0..1\n",
        "display_lbls = le2.classes_\n",
        "\n",
        "# --- Raw counts CM ---\n",
        "cm = confusion_matrix(y_true, y_pred, labels=idx_labels)\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "# --- Row-normalized CM (per true class) ---\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    cm_norm = cm / row_sums\n",
        "\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}_{suffix}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}_{suffix}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08fb3d1e92ec4d7a95e355bf7625e246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29675e91dc38483cb9501914ce9edf80",
              "IPY_MODEL_b4e350cb7bf645d086d2c5156ff974d1",
              "IPY_MODEL_10a68871569c4e2994e4c16a7f888e3c"
            ],
            "layout": "IPY_MODEL_6fa140acc79141e7beb9da64648869be"
          }
        },
        "10a68871569c4e2994e4c16a7f888e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dcebd0f8924d98a9802b9bb198a890",
            "placeholder": "​",
            "style": "IPY_MODEL_2ecef226f25d4564a65dd502e44b217d",
            "value": " 2/2 [00:01&lt;00:00,  2.00it/s]"
          }
        },
        "1b3906efffc442a2ac1344ed646bf7a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29675e91dc38483cb9501914ce9edf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3906efffc442a2ac1344ed646bf7a0",
            "placeholder": "​",
            "style": "IPY_MODEL_8ae0fb820440436995f40fe18bb5ca2c",
            "value": "100%"
          }
        },
        "2ecef226f25d4564a65dd502e44b217d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fa140acc79141e7beb9da64648869be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae0fb820440436995f40fe18bb5ca2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95dcebd0f8924d98a9802b9bb198a890": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d7c56fa50347c0b7abfea9979f4ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4e350cb7bf645d086d2c5156ff974d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca799509779649b68d8ee3d5572a6c70",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4d7c56fa50347c0b7abfea9979f4ea7",
            "value": 2
          }
        },
        "ca799509779649b68d8ee3d5572a6c70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
