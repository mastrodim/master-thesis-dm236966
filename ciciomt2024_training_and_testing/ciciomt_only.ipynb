{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing dependencies and setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# + pyarrow (fast CSV/Parquet IO) + polars (fast ETL) + fastparquet (optional)\n",
        "!pip install pandas numpy tqdm scikit-learn xgboost lightgbm matplotlib pyarrow polars fastparquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import polars as pl\n",
        "\n",
        "folder = os.getcwd()\n",
        "csv_files = [f for f in os.listdir(folder) if f.lower().endswith(\".csv\")]\n",
        "if not csv_files:\n",
        "    raise SystemExit(\"No CSV files found in this folder.\")\n",
        "if len(csv_files) > 1:\n",
        "    raise SystemExit(f\"Expected a single CSV, found {len(csv_files)}: {csv_files}\")\n",
        "\n",
        "src = os.path.join(folder, csv_files[0])\n",
        "dst = os.path.join(folder, os.path.splitext(csv_files[0])[0] + \"_clean.parquet\")\n",
        "\n",
        "# Stream CSV -> Parquet (no full collect)\n",
        "(\n",
        "    pl.scan_csv(\n",
        "        src,\n",
        "        try_parse_dates=True,        # parse date-like columns if present\n",
        "        infer_schema_length=200_000, # safer than tiny sample; 0 = read all rows (slower)\n",
        "        # quote_char=None,           # uncomment only if you know there are NO quoted fields\n",
        "        # ignore_errors=True,        # uncomment if you can drop occasional bad lines\n",
        "    )\n",
        "    .sink_parquet(\n",
        "        dst,\n",
        "        compression=\"zstd\",\n",
        "        compression_level=1,         # fast writes\n",
        "        statistics=False,            # faster; enable if you’ll filter heavily later\n",
        "        maintain_order=False,\n",
        "        # row_group_size=500_000,    # tune if needed for very wide/huge files\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"CSV -> Parquet: {src}  ➜  {dst}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Use the current directory\n",
        "folder_path = os.getcwd()\n",
        "\n",
        "# Find all Parquet files in here\n",
        "parquet_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.parquet')]\n",
        "\n",
        "if not parquet_files:\n",
        "    print(\"No Parquet files found in this folder.\")\n",
        "else:\n",
        "    for parquet_file in parquet_files:\n",
        "        file_path = os.path.join(folder_path, parquet_file)\n",
        "        \n",
        "        # Load Parquet\n",
        "        df = pd.read_parquet(file_path)\n",
        "\n",
        "        # Drop any rows with NaNs\n",
        "        df_cleaned = df.dropna()\n",
        "\n",
        "        # Overwrite the same file (or change the filename if you prefer)\n",
        "        df_cleaned.to_parquet(file_path, index=False)\n",
        "\n",
        "        print(f\"Cleaned {parquet_file} → {len(df_cleaned)} rows kept\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import polars as pl\n",
        "\n",
        "folder = os.getcwd()\n",
        "pattern = os.path.join(folder, \"*_clean.parquet\")\n",
        "out_file = os.path.join(folder, \"label_counts.txt\")\n",
        "\n",
        "# Scan, then select only the Label column (projection pushdown will keep it fast).\n",
        "q = (\n",
        "    pl.scan_parquet(pattern)\n",
        "      .select(pl.col(\"Label\").cast(pl.Categorical))\n",
        "      .filter(pl.col(\"Label\").is_not_null())\n",
        ")\n",
        "\n",
        "# Total rows (collect just a tiny scalar)\n",
        "n_rows = (\n",
        "    q.select(pl.len().alias(\"rows\"))\n",
        "     .collect(engine=\"streaming\")[\"rows\"][0]\n",
        ")\n",
        "\n",
        "# Label counts (materialize only the small result)\n",
        "label_counts = (\n",
        "    q.group_by(\"Label\")\n",
        "     .len()\n",
        "     .sort(\"len\", descending=True)\n",
        "     .collect(engine=\"streaming\")\n",
        "     .rename({\"len\": \"count\"})\n",
        ")\n",
        "\n",
        "# Write to txt\n",
        "with open(out_file, \"w\") as f:\n",
        "    f.write(\"Analyzing original Parquet files (glob): *_clean.parquet\\n\")\n",
        "    f.write(f\"Total rows: {n_rows}\\n\\n\")\n",
        "    f.write(\"Unique labels and their counts:\\n\")\n",
        "    for label, count in zip(label_counts[\"Label\"].to_list(),\n",
        "                            label_counts[\"count\"].to_list()):\n",
        "        f.write(f\"  {label}: {count}\\n\")\n",
        "    f.write(f\"\\nTotal unique labels: {label_counts.shape[0]}\\n\")\n",
        "\n",
        "print(f\"Results written to {out_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "folder = os.getcwd()\n",
        "pattern = os.path.join(folder, \"*_clean.parquet\")\n",
        "\n",
        "# 1) Lazily scan all parts (single call; supports glob)\n",
        "lazy_all = pl.scan_parquet(pattern)\n",
        "\n",
        "# 2) Add a stable row id (after concat) and ensure Label dtype\n",
        "#    We will ONLY collect (rid, Label), not the whole table.\n",
        "lazy_all = lazy_all.with_row_index(\"rid\").with_columns(pl.col(\"Label\").cast(pl.Utf8))\n",
        "\n",
        "# 3) Collect tiny metadata needed for splitting\n",
        "meta = lazy_all.select(\"rid\", \"Label\").collect(engine=\"streaming\")\n",
        "\n",
        "if \"Label\" not in meta.columns:\n",
        "    raise RuntimeError(\"Expected a 'Label' column in the dataset.\")\n",
        "\n",
        "# 4) Decide if we can stratify (≥2 classes and each has ≥2 rows)\n",
        "use_stratify = False\n",
        "counts = meta.group_by(\"Label\").len().select(\"len\").to_series().to_numpy()\n",
        "if meta[\"Label\"].n_unique() >= 2 and (counts >= 2).all():\n",
        "    use_stratify = True\n",
        "\n",
        "# 5) Split on *indices* (NumPy) — tiny and fast\n",
        "rid = meta[\"rid\"].to_numpy()\n",
        "labels = meta[\"Label\"].to_numpy()\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    rid,\n",
        "    test_size=0.20,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=labels if use_stratify else None,\n",
        ")\n",
        "\n",
        "# 6) Build small row-id tables and join (semi) to stream rows to Parquet\n",
        "train_ids = pl.DataFrame({\"rid\": train_idx}).lazy()\n",
        "test_ids  = pl.DataFrame({\"rid\": test_idx}).lazy()\n",
        "\n",
        "lazy_with_id = lazy_all  # already has \"rid\" column\n",
        "\n",
        "lazy_train = lazy_with_id.join(train_ids, on=\"rid\", how=\"semi\")\n",
        "lazy_test  = lazy_with_id.join(test_ids,  on=\"rid\", how=\"semi\")\n",
        "\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "test_parquet  = os.path.join(folder, \"test.parquet\")\n",
        "\n",
        "# 7) Stream directly to Parquet (fast path; no full materialization)\n",
        "#    Use a fast codec/level for speed; tweak as you prefer.\n",
        "lazy_train.sink_parquet(train_parquet, compression=\"zstd\", compression_level=1, statistics=False)\n",
        "lazy_test.sink_parquet(test_parquet,   compression=\"zstd\", compression_level=1, statistics=False)\n",
        "\n",
        "print(\n",
        "    \"Data split complete:\\n\"\n",
        "    f\" • Pattern: {pattern}\\n\"\n",
        "    f\" • Training rows: {len(train_idx)}\\n\"\n",
        "    f\" • Test rows: {len(test_idx)}\\n\"\n",
        "    f\" • Stratified: {use_stratify}\\n\"\n",
        "    f\" • Saved: {train_parquet}, {test_parquet}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We deduce that there are 17(16+1) Classes (Attack Labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "folder = os.getcwd()\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "\n",
        "# 1) Load train\n",
        "train_pl = pl.read_parquet(train_parquet)\n",
        "y_column = \"Label\"\n",
        "if y_column not in train_pl.columns:\n",
        "    raise RuntimeError(\"Expected 'Label' in train.parquet\")\n",
        "\n",
        "# 2) Numeric cols (exclude label)\n",
        "numeric_cols = [c for c in train_pl.select(cs.numeric()).columns if c != y_column]\n",
        "if not numeric_cols:\n",
        "    raise RuntimeError(\"No numeric feature columns found.\")\n",
        "\n",
        "# 3) Medians on TRAIN ONLY\n",
        "meds_s = train_pl.select([pl.col(c).median().alias(c) for c in numeric_cols])\n",
        "medians = {c: float(meds_s[c][0]) for c in numeric_cols}\n",
        "\n",
        "# 4) Impute to float32 → view as float64 for stats\n",
        "def to_imputed_f32(df: pl.DataFrame, cols: list[str], med: dict[str, float]) -> np.ndarray:\n",
        "    df = df.with_columns([pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c) for c in cols])\n",
        "    df = df.with_columns([pl.col(c).fill_null(med[c]).fill_nan(med[c]).cast(pl.Float32).alias(c) for c in cols])\n",
        "    return df.select(cols).to_numpy().astype(np.float32, copy=False)\n",
        "\n",
        "X_train_for_scaler = to_imputed_f32(train_pl, numeric_cols, medians).astype(np.float64, copy=False)\n",
        "\n",
        "# 4.1) Drop zero/near-zero variance cols once\n",
        "stds = X_train_for_scaler.std(axis=0, dtype=np.float64)  # ddof=0 like StandardScaler\n",
        "EPS_STD = 1e-12\n",
        "keep_mask = stds > EPS_STD\n",
        "if not keep_mask.all():\n",
        "    dropped = [c for c, k in zip(numeric_cols, keep_mask) if not k]\n",
        "    print(f\"Dropping {len(dropped)} ~zero-variance cols:\", dropped)\n",
        "    numeric_cols = [c for c, k in zip(numeric_cols, keep_mask) if k]\n",
        "    X_train_for_scaler = X_train_for_scaler[:, keep_mask]\n",
        "    medians = {c: medians[c] for c in numeric_cols}\n",
        "\n",
        "# 5) Fit scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_for_scaler)\n",
        "\n",
        "# 5.1) Save LR keep indices for near-constant scales\n",
        "EPS_SCALE = 1e-6\n",
        "lr_keep_mask = scaler.scale_ >= EPS_SCALE\n",
        "lr_keep_idx = np.where(lr_keep_mask)[0].astype(np.int32)\n",
        "if not lr_keep_mask.all():\n",
        "    dropped_lr = [c for c, k in zip(numeric_cols, lr_keep_mask) if not k]\n",
        "    print(f\"[LR path] Dropping {len(dropped_lr)} near-constant cols by scale:\", dropped_lr)\n",
        "\n",
        "# 6) Save artifacts\n",
        "artifacts = {\n",
        "    \"numeric_cols\": numeric_cols,\n",
        "    \"medians\": medians,\n",
        "    \"scaler_mean\": scaler.mean_.astype(np.float64),\n",
        "    \"scaler_scale\": scaler.scale_.astype(np.float64),  # keep raw\n",
        "    \"lr_keep_idx\": lr_keep_idx,\n",
        "}\n",
        "with open(os.path.join(folder, \"preproc.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(artifacts, f)\n",
        "\n",
        "print(f\"Saved preproc.pkl with {len(numeric_cols)} cols (LR cols kept: {lr_keep_idx.size}).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoJkN86QQLNX"
      },
      "source": [
        "## 17(16+1) Classes (Attack Labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBSTBnT1rUSa",
        "outputId": "e179b652-e24d-4c73-eaee-e6829ddc5253"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# --- Threading: let libs use all cores (don't cap OMP to 1)\n",
        "import os, time, warnings\n",
        "CORES = os.cpu_count() or 4\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(CORES)        # XGBoost / OpenMP users\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(CORES)        # BLAS for LR/NumPy\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(CORES)\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(min(CORES, 8))\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --------------------------\n",
        "# 1) Load train/test Parquet (Polars)\n",
        "# --------------------------\n",
        "folder = os.getcwd()\n",
        "train_parquet = os.path.join(folder, \"train.parquet\")\n",
        "test_parquet  = os.path.join(folder, \"test.parquet\")\n",
        "if not (os.path.exists(train_parquet) and os.path.exists(test_parquet)):\n",
        "    raise RuntimeError(\"Missing train.parquet/test.parquet. Run the splitter step first.\")\n",
        "\n",
        "train_pl = pl.read_parquet(train_parquet)\n",
        "test_pl  = pl.read_parquet(test_parquet)\n",
        "\n",
        "# --------------------------\n",
        "# 2) Columns: numeric features + label\n",
        "# --------------------------\n",
        "y_column = \"Label\"\n",
        "if y_column not in train_pl.columns or y_column not in test_pl.columns:\n",
        "    raise RuntimeError(\"Expected a 'Label' column in both train and test dataframes.\")\n",
        "\n",
        "schema = train_pl.schema\n",
        "numeric_cols = [c for c, dt in schema.items() if c != y_column and dt in pl.NUMERIC_DTYPES]\n",
        "if not numeric_cols:\n",
        "    raise RuntimeError(\"No numeric feature columns found.\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) Feature hygiene in Polars (inf -> null; fill null/nan with median; cast to f32)\n",
        "# --------------------------\n",
        "def prep_numeric(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    # replace ±inf with null\n",
        "    df = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    # compute medians once\n",
        "    meds = df.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    # fill nulls/NaNs with medians and cast to float32\n",
        "    df = df.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df\n",
        "\n",
        "train_pl = prep_numeric(train_pl, numeric_cols)\n",
        "test_pl  = prep_numeric(test_pl,  numeric_cols)\n",
        "\n",
        "# --------------------------\n",
        "# 4) To NumPy (float32, optionally Fortran order)\n",
        "# --------------------------\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full = to_f32_f_order(train_pl, numeric_cols)\n",
        "X_test       = to_f32_f_order(test_pl,  numeric_cols)\n",
        "\n",
        "# Labels\n",
        "y_train_raw = train_pl.select(y_column).to_series().to_numpy()\n",
        "y_test_raw  = test_pl.select(y_column).to_series().to_numpy()\n",
        "\n",
        "# --------------------------\n",
        "# 5) Label encoding on union(train,test) to avoid unseen-class errors\n",
        "# --------------------------\n",
        "le = LabelEncoder()\n",
        "le.fit(np.concatenate([y_train_raw, y_test_raw]).astype(str))\n",
        "y_train_full = le.transform(y_train_raw.astype(str))\n",
        "y_test       = le.transform(y_test_raw.astype(str))\n",
        "\n",
        "# --------------------------\n",
        "# 6) Scaling for LR in float32 (apply StandardScaler params in f32)\n",
        "# --------------------------\n",
        "scaler_path = os.path.join(folder, \"preproc.pkl\")\n",
        "if not os.path.exists(scaler_path):\n",
        "    raise RuntimeError(\"preproc.pkl not found. Run the preprocessing step first.\")\n",
        "with open(scaler_path, \"rb\") as f:\n",
        "    artifacts = pickle.load(f)\n",
        "\n",
        "mean_f32  = np.asarray(artifacts[\"scaler_mean\"],  dtype=np.float32)\n",
        "scale_f32 = np.asarray(artifacts[\"scaler_scale\"], dtype=np.float32)\n",
        "\n",
        "if mean_f32 is None or scale_f32 is None:\n",
        "    raise RuntimeError(\"Loaded scaler lacks mean_/scale_. Refit with StandardScaler.\")\n",
        "scale_safe = np.where(scale_f32 == 0.0, 1.0, scale_f32)\n",
        "\n",
        "def apply_scaler_f32(X_f):\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full = apply_scaler_f32(X_train_full)\n",
        "X_test_lr       = apply_scaler_f32(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# 7) Validation split for early stopping\n",
        "# --------------------------\n",
        "X_tr_num, X_val_num, y_tr, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
        ")\n",
        "X_tr_lr,  X_val_lr  = train_test_split(\n",
        "    X_train_lr_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 8) Models (light defaults) + boosters with early stopping\n",
        "# --------------------------\n",
        "RANDOM_STATE = 42\n",
        "ML_models = [\n",
        "    (\"LogisticRegression\", LogisticRegression(\n",
        "        solver=\"lbfgs\", penalty=\"l2\", max_iter=1000, tol=2e-3, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"DecisionTreeClassifier\", DecisionTreeClassifier(\n",
        "        criterion=\"entropy\", max_depth=5, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"RandomForestClassifier\", RandomForestClassifier(\n",
        "        n_estimators=100, max_features=\"sqrt\", n_jobs=-1, random_state=RANDOM_STATE\n",
        "    )),\n",
        "    (\"AdaBoostClassifier\", AdaBoostClassifier(\n",
        "        n_estimators=50, learning_rate=0.5, random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    (\"BaggingClassifier\", BaggingClassifier(\n",
        "        n_estimators=10, n_jobs=-1, random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    (\"LGBMClassifier\", LGBMClassifier(\n",
        "        n_estimators=4000, num_leaves=48, learning_rate=0.06,\n",
        "        subsample=0.8, colsample_bytree=0.8,\n",
        "        n_jobs=CORES, random_state=RANDOM_STATE\n",
        "    )),\n",
        "]\n",
        "\n",
        "# --------------------------\n",
        "# 9) Report\n",
        "# --------------------------\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "open(report_path, \"w\").close()\n",
        "\n",
        "def log_metrics(name, model, X_te, y_te):\n",
        "    y_pred = model.predict(X_te)\n",
        "    acc  = accuracy_score(y_te, y_pred)\n",
        "    rec  = recall_score(y_te, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_te, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_te, y_pred, average=\"macro\")\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 10) Train / time / save / evaluate\n",
        "# --------------------------\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models\"):\n",
        "    print(f\"\\n▶ Training {name}\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr, y_tr)\n",
        "        Xte = X_test_lr\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        Xte = X_test  # test features for XGB always unscaled numeric\n",
        "\n",
        "    # Try new API (callbacks)\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback  # may not exist on very old versions\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num, y_tr,\n",
        "                eval_set=[(X_val_num, y_val)],\n",
        "                callbacks=es,\n",
        "        )\n",
        "        except (TypeError, ImportError):\n",
        "            # Fallback: older API with early_stopping_rounds in fit\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num, y_tr,\n",
        "                    eval_set=[(X_val_num, y_val)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Last resort: no early stopping\n",
        "                model.fit(X_tr_num, y_tr)   \n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num, y_tr,\n",
        "            eval_set=[(X_val_num, y_val)],\n",
        "            eval_metric=\"multi_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num, y_tr)\n",
        "        Xte = X_test\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save\n",
        "    with open(os.path.join(folder, f\"{name}_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    print(f\"▶ Testing {name}\")\n",
        "    log_metrics(name, model, Xte, y_test)\n",
        "\n",
        "print(f\"\\nAll models trained, tested, and metrics logged to '{report_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for any saved model\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match the filename you saved) ---\n",
        "model_name = \"LGBMClassifier\"        # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "model_path = os.path.join(folder, f\"{model_name}_model.pkl\")\n",
        "\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features\n",
        "Xte = X_test_lr if model_name == \"LogisticRegression\" else X_test\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels:\n",
        "# - y_test is already integer-encoded with LabelEncoder\n",
        "# - Use index range for confusion_matrix, and le.classes_ as display labels\n",
        "idx_labels   = np.arange(len(le.classes_))  # 0..K-1\n",
        "display_lbls = le.classes_\n",
        "\n",
        "# Raw counts CM\n",
        "cm = confusion_matrix(y_test, y_pred, labels=idx_labels)\n",
        "\n",
        "# Save counts CM to CSV\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "# Plot counts CM\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name}\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}.png\"), bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "# Row-normalized (per true class)\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # avoid div/0 if a class is absent\n",
        "    cm_norm = (cm / row_sums)\n",
        "\n",
        "# Save normalized CM to CSV\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "# Plot normalized CM\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name}\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}.png\"), bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import polars as pl\n",
        "\n",
        "# Combine train/test labels\n",
        "df = pl.concat([train_pl.select(\"Label\"), test_pl.select(\"Label\")])\n",
        "\n",
        "# Count labels\n",
        "label_counts = (\n",
        "    df.group_by(\"Label\")\n",
        "      .len()\n",
        "      .sort(\"len\", descending=True)\n",
        ")\n",
        "\n",
        "# Save to TXT\n",
        "out_path = os.path.join(folder, \"label_counts.txt\")\n",
        "with open(out_path, \"w\") as f:\n",
        "    for row in label_counts.iter_rows():\n",
        "        f.write(f\"{row[0]}: {row[1]}\\n\")\n",
        "\n",
        "print(f\"✅ Saved labels and counts to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6(5+1) Classes (Attack Labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We group the attacks in classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XbS_625DT9ui"
      },
      "outputs": [],
      "source": [
        "dict_6classes = {}\n",
        "\n",
        "# --- DDoS ---\n",
        "dict_6classes['DDoS_ICMP'] = 'DDoS'\n",
        "dict_6classes['DDoS_UDP'] = 'DDoS'\n",
        "dict_6classes['DDoS_TCP'] = 'DDoS'\n",
        "dict_6classes['DDoS_SYN'] = 'DDoS'\n",
        "\n",
        "# --- DoS ---\n",
        "dict_6classes['DoS_UDP'] = 'DoS'\n",
        "dict_6classes['DoS_TCP'] = 'DoS'\n",
        "dict_6classes['DoS_SYN'] = 'DoS'\n",
        "dict_6classes['DoS_ICMP'] = 'DoS'\n",
        "\n",
        "# --- Benign ---\n",
        "dict_6classes['Benign'] = 'Benign'\n",
        "\n",
        "# --- Spoofing ---\n",
        "dict_6classes['Spoofing_ARP'] = 'Spoofing'\n",
        "\n",
        "# --- Recon ---\n",
        "dict_6classes['Recon_Ping_Sweep'] = 'Recon'\n",
        "dict_6classes['Recon_OS_Scan'] = 'Recon'\n",
        "dict_6classes['Recon_Port_Scan'] = 'Recon'\n",
        "dict_6classes['Recon_Vulnerability_Scan'] = 'Recon'\n",
        "\n",
        "# --- MQTT ---\n",
        "dict_6classes['MQTT_DDoS_Flooding'] = 'MQTT'\n",
        "dict_6classes['MQTT_DoS_Flooding'] = 'MQTT'\n",
        "dict_6classes['MQTT_Malformed_Data'] = 'MQTT'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6-class grouping run (Polars → NumPy) ===\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import os, pickle, time\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "y_column = \"Label\"\n",
        "\n",
        "# --- Map labels to 6 classes (drop anything unmapped) ---\n",
        "def map_labels_pl(df: pl.DataFrame, mapping: dict) -> pl.DataFrame:\n",
        "    mapped = df.with_columns(\n",
        "        pl.col(y_column).cast(pl.Utf8).replace(mapping, default=None).alias(y_column)\n",
        "    )\n",
        "    # Drop rows where mapping failed\n",
        "    return mapped.drop_nulls(subset=[y_column])\n",
        "\n",
        "train_pl_6 = map_labels_pl(train_pl, dict_6classes)\n",
        "test_pl_6  = map_labels_pl(test_pl,  dict_6classes)\n",
        "\n",
        "# --- Hygiene per split: replace ±inf→null, fill with medians, cast float32 ---\n",
        "def prep_numeric_pl(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_pl_6 = prep_numeric_pl(train_pl_6, numeric_cols)\n",
        "test_pl_6  = prep_numeric_pl(test_pl_6,  numeric_cols)\n",
        "\n",
        "# --- To NumPy (float32, Fortran order) ---\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full_6 = to_f32_f_order(train_pl_6, numeric_cols)\n",
        "X_test_6       = to_f32_f_order(test_pl_6,  numeric_cols)\n",
        "\n",
        "# --- Labels (string → LabelEncoder over union) ---\n",
        "y_train_6_raw = train_pl_6.select(y_column).to_series().to_numpy().astype(str)\n",
        "y_test_6_raw  = test_pl_6.select(y_column).to_series().to_numpy().astype(str)\n",
        "\n",
        "le6 = LabelEncoder()\n",
        "le6.fit(np.concatenate([y_train_6_raw, y_test_6_raw]))\n",
        "y_train_full_6 = le6.transform(y_train_6_raw)\n",
        "y_test_6       = le6.transform(y_test_6_raw)\n",
        "\n",
        "# --- Scaling for LR (reuse your pre-fitted scaler params already loaded) ---\n",
        "def apply_scaler_f32(X_f):\n",
        "    # reusing mean_f32 and scale_safe defined earlier in your session\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full_6 = apply_scaler_f32(X_train_full_6)\n",
        "X_test_lr_6       = apply_scaler_f32(X_test_6)\n",
        "\n",
        "# --- Train/val split (stratified) for early stopping boosters ---\n",
        "X_tr_num6, X_val_num6, y_tr6, y_val6 = train_test_split(\n",
        "    X_train_full_6, y_train_full_6, test_size=0.15, random_state=42, stratify=y_train_full_6\n",
        ")\n",
        "X_tr_lr6,  X_val_lr6  = train_test_split(\n",
        "    X_train_lr_full_6, test_size=0.15, random_state=42, stratify=y_train_full_6\n",
        ")\n",
        "\n",
        "# --- Append header to the same report.txt ---\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "with open(report_path, \"a\") as fp:\n",
        "    fp.write(\"\\n\\n===== Grouped 6-Class Run =====\\n\")\n",
        "    fp.write(\"Classes (encoded order): \" + \", \".join(map(str, le6.classes_)) + \"\\n\")\n",
        "\n",
        "# --- Train / evaluate each model on 6 classes ---\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models (6-class)\"):\n",
        "    print(f\"\\n▶ Training {name} (6-class)\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr6, y_tr6)\n",
        "        Xte = X_test_lr_6\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        # (Only if you have XGB in ML_models; your current list does not include it.)\n",
        "        Xte = X_test_6\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num6, y_tr6,\n",
        "                eval_set=[(X_val_num6, y_val6)],\n",
        "                callbacks=es,\n",
        "            )\n",
        "        except (TypeError, ImportError):\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num6, y_tr6,\n",
        "                    eval_set=[(X_val_num6, y_val6)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                model.fit(X_tr_num6, y_tr6)\n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num6, y_tr6,\n",
        "            eval_set=[(X_val_num6, y_val6)],\n",
        "            eval_metric=\"multi_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test_6\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num6, y_tr6)\n",
        "        Xte = X_test_6\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} (6-class) trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save model\n",
        "    with open(os.path.join(folder, f\"{name}_6classes_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"▶ Testing {name} (6-class)\")\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc  = accuracy_score(y_test_6, y_pred)\n",
        "    rec  = recall_score(y_test_6, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_test_6, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_test_6, y_pred, average=\"macro\")\n",
        "\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} (6 Classes) #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "print(\"\\n✅ All 6-class models trained & logged to 'report.txt'.\")\n",
        "print(\"Encoded class order (6-class):\", list(le6.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for 6-class models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match what you saved with _6classes) ---\n",
        "model_name = \"LGBMClassifier\"        # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "suffix = \"6classes\"                   # keeps filenames distinct\n",
        "\n",
        "model_path = os.path.join(folder, f\"{model_name}_{suffix}_model.pkl\")\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features & labels for 6-class run\n",
        "Xte = X_test_lr_6 if model_name == \"LogisticRegression\" else X_test_6\n",
        "y_true = y_test_6\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels/ticks: use full index range to force consistent matrix shape, display names from le6\n",
        "idx_labels   = np.arange(len(le6.classes_))   # 0..K-1\n",
        "display_lbls = le6.classes_\n",
        "\n",
        "# --- Raw counts CM ---\n",
        "cm = confusion_matrix(y_true, y_pred, labels=idx_labels)\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "# --- Row-normalized CM (per true class) ---\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # avoid div/0 if a class is absent\n",
        "    cm_norm = cm / row_sums\n",
        "\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}_{suffix}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}_{suffix}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2(1+1) Classes (Attack or Benign)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGN2itN2ZpgY"
      },
      "source": [
        "Now, let's move to just 2 classifiers(attack, benign)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bQAp_f24ZuKI"
      },
      "outputs": [],
      "source": [
        "dict_2classes = {}\n",
        "\n",
        "# --- Benign ---\n",
        "dict_2classes['Benign'] = 'Benign'\n",
        "\n",
        "# --- DDoS ---\n",
        "dict_2classes['DDoS_ICMP'] = 'Attack'\n",
        "dict_2classes['DDoS_UDP'] = 'Attack'\n",
        "dict_2classes['DDoS_TCP'] = 'Attack'\n",
        "dict_2classes['DDoS_SYN'] = 'Attack'\n",
        "\n",
        "# --- DoS ---\n",
        "dict_2classes['DoS_UDP'] = 'Attack'\n",
        "dict_2classes['DoS_TCP'] = 'Attack'\n",
        "dict_2classes['DoS_SYN'] = 'Attack'\n",
        "dict_2classes['DoS_ICMP'] = 'Attack'\n",
        "\n",
        "# --- Spoofing ---\n",
        "dict_2classes['Spoofing_ARP'] = 'Attack'\n",
        "\n",
        "# --- Recon ---\n",
        "dict_2classes['Recon_Ping_Sweep'] = 'Attack'\n",
        "dict_2classes['Recon_OS_Scan'] = 'Attack'\n",
        "dict_2classes['Recon_Port_Scan'] = 'Attack'\n",
        "dict_2classes['Recon_Vulnerability_Scan'] = 'Attack'\n",
        "\n",
        "# --- MQTT ---\n",
        "dict_2classes['MQTT_DDoS_Flooding'] = 'Attack'\n",
        "dict_2classes['MQTT_DoS_Flooding'] = 'Attack'\n",
        "dict_2classes['MQTT_Malformed_Data'] = 'Attack'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2-class grouping run (Polars → NumPy) ===\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import os, pickle, time\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_column = \"Label\"\n",
        "\n",
        "# --- Map labels to 2 classes (drop anything unmapped) ---\n",
        "def map_labels_pl(df: pl.DataFrame, mapping: dict) -> pl.DataFrame:\n",
        "    mapped = df.with_columns(\n",
        "        pl.col(y_column).cast(pl.Utf8).replace(mapping, default=None).alias(y_column)\n",
        "    )\n",
        "    return mapped.drop_nulls(subset=[y_column])\n",
        "\n",
        "train_pl_2 = map_labels_pl(train_pl, dict_2classes)\n",
        "test_pl_2  = map_labels_pl(test_pl,  dict_2classes)\n",
        "\n",
        "# --- Hygiene per split: ±inf→null, fill medians, cast float32 ---\n",
        "def prep_numeric_pl(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_pl_2 = prep_numeric_pl(train_pl_2, numeric_cols)\n",
        "test_pl_2  = prep_numeric_pl(test_pl_2,  numeric_cols)\n",
        "\n",
        "# --- To NumPy (float32, Fortran order) ---\n",
        "def to_f32_f_order(df: pl.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    arr = df.select(cols).to_numpy()\n",
        "    return np.asfortranarray(arr, dtype=np.float32)\n",
        "\n",
        "X_train_full_2 = to_f32_f_order(train_pl_2, numeric_cols)\n",
        "X_test_2       = to_f32_f_order(test_pl_2,  numeric_cols)\n",
        "\n",
        "# --- Labels (string → LabelEncoder over union) ---\n",
        "y_train_2_raw = train_pl_2.select(y_column).to_series().to_numpy().astype(str)\n",
        "y_test_2_raw  = test_pl_2.select(y_column).to_series().to_numpy().astype(str)\n",
        "\n",
        "le2 = LabelEncoder()\n",
        "le2.fit(np.concatenate([y_train_2_raw, y_test_2_raw]))\n",
        "y_train_full_2 = le2.transform(y_train_2_raw)\n",
        "y_test_2       = le2.transform(y_test_2_raw)\n",
        "\n",
        "# --- Scaling for LR (reuse your pre-fitted scaler params already loaded) ---\n",
        "def apply_scaler_f32(X_f):\n",
        "    out = np.empty_like(X_f, dtype=np.float32, order='F')\n",
        "    np.subtract(X_f, mean_f32, out=out)\n",
        "    np.divide(out, scale_safe, out=out)\n",
        "    return out\n",
        "\n",
        "X_train_lr_full_2 = apply_scaler_f32(X_train_full_2)\n",
        "X_test_lr_2       = apply_scaler_f32(X_test_2)\n",
        "\n",
        "# --- Train/val split (stratified) for early stopping boosters ---\n",
        "X_tr_num2, X_val_num2, y_tr2, y_val2 = train_test_split(\n",
        "    X_train_full_2, y_train_full_2, test_size=0.15, random_state=42, stratify=y_train_full_2\n",
        ")\n",
        "X_tr_lr2,  X_val_lr2  = train_test_split(\n",
        "    X_train_lr_full_2, test_size=0.15, random_state=42, stratify=y_train_full_2\n",
        ")\n",
        "\n",
        "# --- Append header to the same report.txt ---\n",
        "report_path = os.path.join(folder, \"report.txt\")\n",
        "with open(report_path, \"a\") as fp:\n",
        "    fp.write(\"\\n\\n===== Grouped 2-Class Run =====\\n\")\n",
        "    fp.write(\"Classes (encoded order): \" + \", \".join(map(str, le2.classes_)) + \"\\n\")\n",
        "\n",
        "# --- Train / evaluate each model on 2 classes ---\n",
        "for name, model in tqdm(ML_models, total=len(ML_models), desc=\"Models (2-class)\"):\n",
        "    print(f\"\\n▶ Training {name} (2-class)\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    if name == \"LogisticRegression\":\n",
        "        model.fit(X_tr_lr2, y_tr2)\n",
        "        Xte = X_test_lr_2\n",
        "\n",
        "    elif name == \"XGBClassifier\":\n",
        "        # Only if XGB is in your ML_models\n",
        "        Xte = X_test_2\n",
        "        try:\n",
        "            from xgboost import callback as xgb_callback\n",
        "            es = [xgb_callback.EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "            model.fit(\n",
        "                X_tr_num2, y_tr2,\n",
        "                eval_set=[(X_val_num2, y_val2)],\n",
        "                callbacks=es,\n",
        "            )\n",
        "        except (TypeError, ImportError):\n",
        "            try:\n",
        "                model.fit(\n",
        "                    X_tr_num2, y_tr2,\n",
        "                    eval_set=[(X_val_num2, y_val2)],\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose=False,\n",
        "                )\n",
        "            except TypeError:\n",
        "                model.fit(X_tr_num2, y_tr2)\n",
        "\n",
        "    elif name == \"LGBMClassifier\":\n",
        "        import lightgbm as lgb\n",
        "        model.fit(\n",
        "            X_tr_num2, y_tr2,\n",
        "            eval_set=[(X_val_num2, y_val2)],\n",
        "            eval_metric=\"binary_logloss\",\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "        )\n",
        "        Xte = X_test_2\n",
        "\n",
        "    else:\n",
        "        model.fit(X_tr_num2, y_tr2)\n",
        "        Xte = X_test_2\n",
        "\n",
        "    secs = time.perf_counter() - t0\n",
        "    print(f\"⏱ {name} (2-class) trained in {secs:.1f}s\")\n",
        "\n",
        "    # Save model\n",
        "    with open(os.path.join(folder, f\"{name}_2classes_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"▶ Testing {name} (2-class)\")\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc  = accuracy_score(y_test_2, y_pred)\n",
        "    rec  = recall_score(y_test_2, y_pred, average=\"macro\")\n",
        "    prec = precision_score(y_test_2, y_pred, average=\"macro\")\n",
        "    f1   = f1_score(y_test_2, y_pred, average=\"macro\")\n",
        "\n",
        "    with open(report_path, \"a\") as fp:\n",
        "        fp.write(f\"####### {name} (2 Classes) #######\\n\")\n",
        "        fp.write(f\"Accuracy : {acc:.4f}\\n\")\n",
        "        fp.write(f\"Recall   : {rec:.4f}\\n\")\n",
        "        fp.write(f\"Precision: {prec:.4f}\\n\")\n",
        "        fp.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "\n",
        "print(\"\\n✅ All 2-class models trained & logged to 'report.txt'.\")\n",
        "print(\"Encoded class order (2-class):\", list(le2.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (counts + row-normalized) for 2-class models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "\n",
        "# --- choose the model you want to visualize (must match what you saved with _2classes) ---\n",
        "model_name = \"LGBMClassifier\"   # e.g. \"LogisticRegression\", \"RandomForestClassifier\", ...\n",
        "suffix = \"2classes\"             # keeps filenames distinct\n",
        "\n",
        "model_path = os.path.join(folder, f\"{model_name}_{suffix}_model.pkl\")\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Pick the correct test features & labels for 2-class run\n",
        "Xte    = X_test_lr_2 if model_name == \"LogisticRegression\" else X_test_2\n",
        "y_true = y_test_2\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(Xte)\n",
        "\n",
        "# Labels/ticks: use full index range to force consistent matrix shape; display names from le2\n",
        "idx_labels   = np.arange(len(le2.classes_))   # 0..1\n",
        "display_lbls = le2.classes_\n",
        "\n",
        "# --- Raw counts CM ---\n",
        "cm = confusion_matrix(y_true, y_pred, labels=idx_labels)\n",
        "np.savetxt(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\"\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (counts) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_counts_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "# --- Row-normalized CM (per true class) ---\n",
        "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    cm_norm = cm / row_sums\n",
        "\n",
        "np.savetxt(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.csv\"), cm_norm, fmt=\"%.4f\", delimiter=\",\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8), dpi=200)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_lbls).plot(\n",
        "    ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\", cmap=plt.cm.Blues\n",
        ")\n",
        "ax.set_title(f\"Confusion Matrix (row-normalized) — {model_name} ({suffix})\")\n",
        "plt.savefig(os.path.join(folder, f\"cm_normalized_{model_name}_{suffix}.png\"), bbox_inches=\"tight\")\n",
        "plt.show(); plt.close(fig)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"cm_counts_{model_name}_{suffix}.png / .csv\",\n",
        "      f\"cm_normalized_{model_name}_{suffix}.png / .csv\",\n",
        "      sep=\"\\n- \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metrics for LR, RF, Adaboost, Bagging,  DT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, glob, pickle, gc\n",
        "import numpy as np, polars as pl, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ====== CONFIG ======\n",
        "MODELS_DIR   = \".\"              # folder with your *.pkl models\n",
        "TRAIN_PATH   = \"train.parquet\"\n",
        "TEST_PATH    = \"test.parquet\"\n",
        "PREPROC_PATH = \"preproc.pkl\"    # optional: contains {'scaler_mean','scaler_scale'}\n",
        "LABEL_COL    = \"Label\"\n",
        "SAVE_CSV     = \"overfitting_report.csv\"  # set None to avoid saving\n",
        "# ====================\n",
        "\n",
        "# ---------- Load data ----------\n",
        "train_df = pl.read_parquet(TRAIN_PATH)\n",
        "test_df  = pl.read_parquet(TEST_PATH)\n",
        "\n",
        "# numeric columns only\n",
        "num_dtypes = {pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64}\n",
        "feat_cols_all = [c for c, dt in train_df.schema.items() if c != LABEL_COL and dt in num_dtypes]\n",
        "\n",
        "# ---------- Clean once (±inf -> null -> median) and cast to float32 ----------\n",
        "def clean_float32(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_df = clean_float32(train_df, feat_cols_all)\n",
        "test_df  = clean_float32(test_df,  feat_cols_all)\n",
        "\n",
        "# ---------- Convert once to NumPy float32 (no copies later; we slice by indices) ----------\n",
        "X_train_all = train_df.select(feat_cols_all).to_numpy().astype(np.float32, copy=False)\n",
        "X_test_all  = test_df.select(feat_cols_all).to_numpy().astype(np.float32, copy=False)\n",
        "y_train_raw = train_df[LABEL_COL].to_numpy()\n",
        "y_test_raw  = test_df[LABEL_COL].to_numpy()\n",
        "\n",
        "# ---------- Optional scaler for linear models (precompute scaled buffers once) ----------\n",
        "scaler = None\n",
        "X_train_scaled = None\n",
        "X_test_scaled  = None\n",
        "if os.path.exists(PREPROC_PATH):\n",
        "    with open(PREPROC_PATH, \"rb\") as f:\n",
        "        art = pickle.load(f)\n",
        "    mean = art.get(\"scaler_mean\")\n",
        "    scale = art.get(\"scaler_scale\")\n",
        "    if mean is not None and scale is not None:\n",
        "        mean  = np.asarray(mean,  dtype=np.float32)\n",
        "        scale = np.asarray(scale, dtype=np.float32)\n",
        "        scale_safe = np.where(scale == 0.0, 1.0, scale)\n",
        "        def apply_scaler_inplace(X):\n",
        "            X -= mean\n",
        "            X /= scale_safe\n",
        "        # create scaled copies once (Fortran order helps some estimators)\n",
        "        X_train_scaled = X_train_all.copy(order='F')\n",
        "        X_test_scaled  = X_test_all.copy(order='F')\n",
        "        apply_scaler_inplace(X_train_scaled)\n",
        "        apply_scaler_inplace(X_test_scaled)\n",
        "\n",
        "# ---------- Label encoder on union ----------\n",
        "le_union = LabelEncoder()\n",
        "le_union.fit(np.concatenate([y_train_raw, y_test_raw]).astype(str))\n",
        "y_train_num = le_union.transform(y_train_raw.astype(str))\n",
        "y_test_num  = le_union.transform(y_test_raw.astype(str))\n",
        "\n",
        "# ---------- Fast column index alignment (no Polars → NumPy per model) ----------\n",
        "col_index = {c: i for i, c in enumerate(feat_cols_all)}\n",
        "def align_indices(model, cols_all, col_index_map):\n",
        "    model_cols = getattr(model, \"feature_names_in_\", None)\n",
        "    if model_cols is None:\n",
        "        return np.arange(len(cols_all), dtype=np.int32)\n",
        "    # only keep columns that exist (defensive)\n",
        "    idxs = [col_index_map[c] for c in model_cols if c in col_index_map]\n",
        "    return np.asarray(idxs, dtype=np.int32)\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Collect candidate .pkl model files\n",
        "all_pkls = sorted(glob.glob(os.path.join(MODELS_DIR, \"*.pkl\")))\n",
        "preproc_name = Path(PREPROC_PATH).name.lower()\n",
        "\n",
        "# Exclude any LGBM* and preproc.pkl by filename (same behavior you had)\n",
        "pkls = [\n",
        "    p for p in all_pkls\n",
        "    if not Path(p).name.lower().startswith(\"lgbmclassifier\")\n",
        "    and Path(p).name.lower() != preproc_name\n",
        "]\n",
        "\n",
        "if not pkls:\n",
        "    print(\"No valid .pkl model files found in\", MODELS_DIR)\n",
        "\n",
        "for pkl_path in pkls:\n",
        "    try:\n",
        "        with open(pkl_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "        model_name = Path(pkl_path).name\n",
        "\n",
        "        # Column alignment by indices (zero-copy views)\n",
        "        idxs = align_indices(model, feat_cols_all, col_index)\n",
        "\n",
        "        # Choose scaled or unscaled buffers (no copying)\n",
        "        is_linear = any(k in type(model).__name__.lower() for k in [\"logistic\", \"sgd\", \"linear\"])\n",
        "        use_scaled = is_linear and (X_train_scaled is not None)\n",
        "        Xtr_full = (X_train_scaled if use_scaled else X_train_all)[:, idxs]\n",
        "        Xte_full = (X_test_scaled  if use_scaled else X_test_all )[:, idxs]\n",
        "\n",
        "        # Predict to infer label type (full train & test; no sampling)\n",
        "        y_pred_tr = model.predict(Xtr_full)\n",
        "        y_pred_te = model.predict(Xte_full)\n",
        "\n",
        "        # Match y_true type to y_pred type\n",
        "        if np.issubdtype(np.asarray(y_pred_tr).dtype, np.number):\n",
        "            y_tr = y_train_num\n",
        "            y_te = y_test_num\n",
        "        else:\n",
        "            y_tr = y_train_raw.astype(str)\n",
        "            y_te = y_test_raw.astype(str)\n",
        "            y_pred_tr = y_pred_tr.astype(str)\n",
        "            y_pred_te = y_pred_te.astype(str)\n",
        "\n",
        "        # Metrics\n",
        "        acc_tr = accuracy_score(y_tr, y_pred_tr)\n",
        "        acc_te = accuracy_score(y_te, y_pred_te)\n",
        "        f1_tr  = f1_score(y_tr, y_pred_tr, average=\"macro\")\n",
        "        f1_te  = f1_score(y_te, y_pred_te, average=\"macro\")\n",
        "\n",
        "        # Log-loss (both train & test) only if predict_proba exists\n",
        "        ll_tr = ll_te = np.nan\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            try:\n",
        "                proba_tr = model.predict_proba(Xtr_full)\n",
        "                proba_te = model.predict_proba(Xte_full)\n",
        "                # Use label sets derived from y_* to satisfy log_loss label ordering\n",
        "                ll_tr = log_loss(y_tr, proba_tr, labels=np.unique(y_tr))\n",
        "                ll_te = log_loss(y_te, proba_te, labels=np.unique(y_te))\n",
        "                # free large arrays ASAP\n",
        "                del proba_tr, proba_te\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        gap = acc_tr - acc_te\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"n_features_used\": int(len(idxs)),\n",
        "            \"train_acc\": acc_tr, \"test_acc\": acc_te, \"acc_gap\": gap,\n",
        "            \"train_f1_macro\": f1_tr, \"test_f1_macro\": f1_te,\n",
        "            \"train_logloss\": ll_tr, \"test_logloss\": ll_te\n",
        "        })\n",
        "\n",
        "        # Free per-model temps\n",
        "        del Xtr_full, Xte_full, y_pred_tr, y_pred_te\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        rows.append({\n",
        "            \"model\": Path(pkl_path).name,\n",
        "            \"n_features_used\": np.nan,\n",
        "            \"train_acc\": np.nan, \"test_acc\": np.nan, \"acc_gap\": np.nan,\n",
        "            \"train_f1_macro\": np.nan, \"test_f1_macro\": np.nan,\n",
        "            \"train_logloss\": np.nan, \"test_logloss\": np.nan,\n",
        "            \"error\": repr(e)\n",
        "        })\n",
        "\n",
        "report = pd.DataFrame(rows).sort_values([\"acc_gap\"], ascending=False).reset_index(drop=True)\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "display(report)\n",
        "\n",
        "if SAVE_CSV:\n",
        "    report.to_csv(SAVE_CSV, index=False)\n",
        "    print(f\"\\nSaved report to {SAVE_CSV}\")\n",
        "\n",
        "# Quick textual flag\n",
        "flagged = report[(report[\"acc_gap\"].notna()) & (report[\"acc_gap\"] > 0.05)]\n",
        "print(f\"\\nModels with potential overfitting (acc_gap > 0.05): {len(flagged)}\")\n",
        "for m in flagged[\"model\"].tolist():\n",
        "    print(\"  •\", m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metrics for LGBM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os, numpy as np, polars as pl, matplotlib.pyplot as plt, lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# ==========================================================\n",
        "#                 LABEL GROUPING DICTIONARIES\n",
        "# ==========================================================\n",
        "dict_6classes = {\n",
        "    # --- DDoS ---\n",
        "    'DDoS_ICMP': 'DDoS', 'DDoS_UDP': 'DDoS', 'DDoS_TCP': 'DDoS', 'DDoS_SYN': 'DDoS',\n",
        "    # --- DoS ---\n",
        "    'DoS_UDP': 'DoS', 'DoS_TCP': 'DoS', 'DoS_SYN': 'DoS', 'DoS_ICMP': 'DoS',\n",
        "    # --- Benign ---\n",
        "    'Benign': 'Benign',\n",
        "    # --- Spoofing ---\n",
        "    'Spoofing_ARP': 'Spoofing',\n",
        "    # --- Recon ---\n",
        "    'Recon_Ping_Sweep': 'Recon', 'Recon_OS_Scan': 'Recon',\n",
        "    'Recon_Port_Scan': 'Recon', 'Recon_Vulnerability_Scan': 'Recon',\n",
        "    # --- MQTT ---\n",
        "    'MQTT_DDoS_Flooding': 'MQTT', 'MQTT_DoS_Flooding': 'MQTT', 'MQTT_Malformed_Data': 'MQTT'\n",
        "}\n",
        "\n",
        "dict_2classes = {\n",
        "    'Benign': 'Benign',\n",
        "    'DDoS_ICMP': 'Attack', 'DDoS_UDP': 'Attack', 'DDoS_TCP': 'Attack', 'DDoS_SYN': 'Attack',\n",
        "    'DoS_UDP': 'Attack', 'DoS_TCP': 'Attack', 'DoS_SYN': 'Attack', 'DoS_ICMP': 'Attack',\n",
        "    'Spoofing_ARP': 'Attack',\n",
        "    'Recon_Ping_Sweep': 'Attack', 'Recon_OS_Scan': 'Attack',\n",
        "    'Recon_Port_Scan': 'Attack', 'Recon_Vulnerability_Scan': 'Attack',\n",
        "    'MQTT_DDoS_Flooding': 'Attack', 'MQTT_DoS_Flooding': 'Attack', 'MQTT_Malformed_Data': 'Attack'\n",
        "}\n",
        "\n",
        "# ==========================================================\n",
        "#                   DATA LOADING & CLEANING\n",
        "# ==========================================================\n",
        "LABEL = \"Label\"\n",
        "train = pl.read_parquet(\"train.parquet\")\n",
        "test  = pl.read_parquet(\"test.parquet\")\n",
        "\n",
        "num_cols = [c for c, dt in train.schema.items() if c != LABEL and dt in pl.NUMERIC_DTYPES]\n",
        "\n",
        "def clean_numeric(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    df2 = df.with_columns([\n",
        "        pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n",
        "        for c in num_cols\n",
        "    ])\n",
        "    meds = df2.select([pl.col(c).median().alias(c) for c in num_cols])\n",
        "    df2 = df2.with_columns([\n",
        "        pl.col(c).fill_null(meds[c][0]).fill_nan(meds[c][0]).cast(pl.Float32).alias(c)\n",
        "        for c in num_cols\n",
        "    ])\n",
        "    return df2\n",
        "\n",
        "train_clean = clean_numeric(train)\n",
        "test_clean  = clean_numeric(test)\n",
        "\n",
        "# ==========================================================\n",
        "#                      HELPER FUNCTIONS\n",
        "# ==========================================================\n",
        "def map_labels(df: pl.DataFrame, mapping: dict | None) -> pl.DataFrame:\n",
        "    if mapping is None:\n",
        "        return df\n",
        "    return df.with_columns(\n",
        "        pl.col(LABEL).cast(pl.Utf8).replace(mapping, default=None).alias(LABEL)\n",
        "    ).drop_nulls(subset=[LABEL])\n",
        "\n",
        "def prepare_Xy(train_df: pl.DataFrame, test_df: pl.DataFrame, mapping: dict | None):\n",
        "    tr_m = map_labels(train_df, mapping)\n",
        "    te_m = map_labels(test_df, mapping)\n",
        "    tr_c = clean_numeric(tr_m)\n",
        "    te_c = clean_numeric(te_m)\n",
        "\n",
        "    X_full = tr_c.select(num_cols).to_numpy().astype(np.float32)\n",
        "    y_full = tr_c.select(LABEL).to_series().to_numpy().astype(str)\n",
        "    X_test = te_c.select(num_cols).to_numpy().astype(np.float32)\n",
        "    y_test = te_c.select(LABEL).to_series().to_numpy().astype(str)\n",
        "\n",
        "    le = LabelEncoder().fit(np.concatenate([y_full, y_test]))\n",
        "    y_full_enc = le.transform(y_full)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X_full, y_full_enc, test_size=0.15, random_state=42, stratify=y_full_enc\n",
        "    )\n",
        "    return X_tr, X_val, y_tr, y_val, X_test, y_test_enc, le\n",
        "\n",
        "def lightgbm_fit_and_curves(X_tr, y_tr, X_val, y_val, X_te, y_te, tag: str, base_params: dict):\n",
        "    n_classes = int(len(np.unique(y_tr)))\n",
        "    params = dict(base_params)\n",
        "    if n_classes == 2:\n",
        "        params.update(dict(objective=\"binary\"))\n",
        "        eval_metric = [\"binary_logloss\", \"binary_error\"]\n",
        "    else:\n",
        "        params.update(dict(objective=\"multiclass\", num_class=n_classes))\n",
        "        eval_metric = [\"multi_logloss\", \"multi_error\"]\n",
        "\n",
        "    model = LGBMClassifier(**params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
        "        eval_metric=eval_metric,\n",
        "        callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "    )\n",
        "\n",
        "    res = model.evals_result_\n",
        "    k0, k1 = list(res.keys())[:2]\n",
        "\n",
        "    def pick(d, opts):\n",
        "        for o in opts:\n",
        "            if o in d: return o\n",
        "        raise KeyError(f\"Metric not found. Have: {list(d.keys())}\")\n",
        "\n",
        "    loss_name = pick(res[k0], [\"multi_logloss\",\"binary_logloss\",\"logloss\"])\n",
        "    err_name  = pick(res[k0], [\"multi_error\",\"binary_error\",\"error\"])\n",
        "\n",
        "    train_loss = np.array(res[k0][loss_name], dtype=float)\n",
        "    val_loss   = np.array(res[k1][loss_name], dtype=float)\n",
        "    train_acc  = 1.0 - np.array(res[k0][err_name], dtype=float)\n",
        "    val_acc    = 1.0 - np.array(res[k1][err_name], dtype=float)\n",
        "\n",
        "    epochs  = np.arange(1, len(train_loss)+1)\n",
        "    best_it = getattr(model, \"best_iteration_\", len(epochs))\n",
        "\n",
        "    # ---- Metrics on validation & test ----\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_te_pred  = model.predict(X_te)\n",
        "\n",
        "    metrics_val = dict(\n",
        "        acc=accuracy_score(y_val, y_val_pred),\n",
        "        rec=recall_score(y_val, y_val_pred, average=\"macro\"),\n",
        "        prec=precision_score(y_val, y_val_pred, average=\"macro\"),\n",
        "        f1=f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "    )\n",
        "    metrics_test = dict(\n",
        "        acc=accuracy_score(y_te, y_te_pred),\n",
        "        rec=recall_score(y_te, y_te_pred, average=\"macro\"),\n",
        "        prec=precision_score(y_te, y_te_pred, average=\"macro\"),\n",
        "        f1=f1_score(y_te, y_te_pred, average=\"macro\")\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== {tag.upper()} RESULTS =====\")\n",
        "    print(f\"Classes: {n_classes} | Best iteration: {best_it}\")\n",
        "    print(f\"Validation  - Acc: {metrics_val['acc']:.4f} | Rec: {metrics_val['rec']:.4f} | \"\n",
        "          f\"Prec: {metrics_val['prec']:.4f} | F1: {metrics_val['f1']:.4f}\")\n",
        "    print(f\"Test        - Acc: {metrics_test['acc']:.4f} | Rec: {metrics_test['rec']:.4f} | \"\n",
        "          f\"Prec: {metrics_test['prec']:.4f} | F1: {metrics_test['f1']:.4f}\")\n",
        "\n",
        "    # ---- Plot training curves ----\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "    ax = axes[0]\n",
        "    ax.plot(epochs, train_acc, label=\"Train Acc\")\n",
        "    ax.plot(epochs, val_acc,   label=\"Val Acc\")\n",
        "    ax.axvline(best_it, linestyle=\"--\", alpha=0.7, label=f\"best_iter={best_it}\")\n",
        "    ax.set_title(f\"[{tag}] Accuracy\"); ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy\")\n",
        "    ax.grid(True, alpha=0.3); ax.legend(loc=\"lower right\")\n",
        "\n",
        "    ax = axes[1]\n",
        "    ax.plot(epochs, train_loss, label=\"Train Loss\")\n",
        "    ax.plot(epochs, val_loss,   label=\"Val Loss\")\n",
        "    ax.axvline(best_it, linestyle=\"--\", alpha=0.7, label=f\"best_iter={best_it}\")\n",
        "    ax.set_title(f\"[{tag}] Loss\"); ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss\")\n",
        "    ax.grid(True, alpha=0.3); ax.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_path = f\"lgbm_training_validation_curves_{tag}.png\"\n",
        "    plt.savefig(out_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    return {\n",
        "        \"tag\": tag,\n",
        "        \"best_iter\": int(best_it),\n",
        "        \"epochs\": epochs,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"figure_path\": out_path,\n",
        "        \"n_classes\": n_classes,\n",
        "        \"val_metrics\": metrics_val,\n",
        "        \"test_metrics\": metrics_test,\n",
        "    }\n",
        "\n",
        "# ==========================================================\n",
        "#                 TRAINING CONFIG & EXECUTION\n",
        "# ==========================================================\n",
        "BASE_PARAMS = dict(\n",
        "    n_estimators=4000,\n",
        "    num_leaves=48,\n",
        "    learning_rate=0.06,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_jobs=os.cpu_count() or 4,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "experiments = [\n",
        "    (\"orig\", None),\n",
        "    (\"6c\",   dict_6classes),\n",
        "    (\"2c\",   dict_2classes),\n",
        "]\n",
        "\n",
        "all_runs = []\n",
        "for tag, mapping in experiments:\n",
        "    X_tr, X_val, y_tr, y_val, X_te, y_te, le = prepare_Xy(train_clean, test_clean, mapping)\n",
        "    run = lightgbm_fit_and_curves(X_tr, y_tr, X_val, y_val, X_te, y_te, tag, BASE_PARAMS)\n",
        "    all_runs.append(run)\n",
        "    print(f\"Saved: {run['figure_path']} | tag={tag} | best_iter={run['best_iter']}\")\n",
        "\n",
        "# ==========================================================\n",
        "#                 COMPARISON FIGURE (VAL CURVES)\n",
        "# ==========================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "for run in all_runs:\n",
        "    axes[0].plot(run[\"epochs\"], run[\"val_acc\"], label=f\"{run['tag']} (best={run['best_iter']})\")\n",
        "axes[0].set_title(\"Validation Accuracy (orig vs 6c vs 2c)\")\n",
        "axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].grid(True, alpha=0.3); axes[0].legend(loc=\"lower right\")\n",
        "\n",
        "for run in all_runs:\n",
        "    axes[1].plot(run[\"epochs\"], run[\"val_loss\"], label=f\"{run['tag']}\")\n",
        "axes[1].set_title(\"Validation Loss (orig vs 6c vs 2c)\")\n",
        "axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Loss\")\n",
        "axes[1].grid(True, alpha=0.3); axes[1].legend(loc=\"upper right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "cmp_path = \"lgbm_validation_curves_comparison.png\"\n",
        "plt.savefig(cmp_path, dpi=160, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison figure saved:\", cmp_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Train–Val Difference Plots for each scheme + comparison ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Per-scheme gap plots ----\n",
        "for run in all_runs:\n",
        "    tag       = run[\"tag\"]\n",
        "    epochs    = run[\"epochs\"]\n",
        "    best_it   = run[\"best_iter\"]\n",
        "    train_acc = run[\"train_acc\"]\n",
        "    val_acc   = run[\"val_acc\"]\n",
        "    train_loss= run[\"train_loss\"]\n",
        "    val_loss  = run[\"val_loss\"]\n",
        "\n",
        "    acc_diff  = train_acc - val_acc          # positive => train > val (possible overfit)\n",
        "    loss_diff = val_loss - train_loss        # positive => val loss > train loss (possible overfit)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Accuracy gap\n",
        "    ax = axes[0]\n",
        "    ax.plot(epochs, acc_diff, label=\"Train - Val Accuracy Gap\")\n",
        "    ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "    ax.axvline(best_it, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"best_iter={best_it}\")\n",
        "    ax.set_title(f\"[{tag}] Accuracy Difference (Train - Validation)\")\n",
        "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy Gap\")\n",
        "    ax.grid(True, alpha=0.3); ax.legend(loc=\"upper right\")\n",
        "    if acc_diff.size:\n",
        "        ax.set_ylim(np.min(acc_diff)*1.1, np.max(acc_diff)*1.1)\n",
        "\n",
        "    # Loss gap\n",
        "    ax = axes[1]\n",
        "    ax.plot(epochs, loss_diff, label=\"Validation - Train Loss Gap\")\n",
        "    ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "    ax.axvline(best_it, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"best_iter={best_it}\")\n",
        "    ax.set_title(f\"[{tag}] Loss Difference (Validation - Train)\")\n",
        "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss Gap\")\n",
        "    ax.grid(True, alpha=0.3); ax.legend(loc=\"upper right\")\n",
        "    if loss_diff.size:\n",
        "        ax.set_ylim(np.min(loss_diff)*1.1, np.max(loss_diff)*1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_path = f\"lgbm_train_val_diff_curves_{tag}.png\"\n",
        "    plt.savefig(out_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "# ---- Combined comparison overlay (val curves gaps across schemes) ----\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for run in all_runs:\n",
        "    tag = run[\"tag\"]\n",
        "    acc_diff  = run[\"train_acc\"] - run[\"val_acc\"]\n",
        "    loss_diff = run[\"val_loss\"]  - run[\"train_loss\"]\n",
        "\n",
        "    axes[0].plot(run[\"epochs\"], acc_diff, label=f\"{tag} (best={run['best_iter']})\")\n",
        "    axes[1].plot(run[\"epochs\"], loss_diff, label=f\"{tag}\")\n",
        "\n",
        "axes[0].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "axes[0].set_title(\"Accuracy Gap: Train - Val (orig vs 6c vs 2c)\")\n",
        "axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Accuracy Gap\")\n",
        "axes[0].grid(True, alpha=0.3); axes[0].legend(loc=\"upper right\")\n",
        "\n",
        "axes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "axes[1].set_title(\"Loss Gap: Val - Train (orig vs 6c vs 2c)\")\n",
        "axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Loss Gap\")\n",
        "axes[1].grid(True, alpha=0.3); axes[1].legend(loc=\"upper right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "cmp_gap_path = \"lgbm_train_val_diff_curves_comparison.png\"\n",
        "plt.savefig(cmp_gap_path, dpi=160, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"Saved:\", cmp_gap_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08fb3d1e92ec4d7a95e355bf7625e246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29675e91dc38483cb9501914ce9edf80",
              "IPY_MODEL_b4e350cb7bf645d086d2c5156ff974d1",
              "IPY_MODEL_10a68871569c4e2994e4c16a7f888e3c"
            ],
            "layout": "IPY_MODEL_6fa140acc79141e7beb9da64648869be"
          }
        },
        "10a68871569c4e2994e4c16a7f888e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dcebd0f8924d98a9802b9bb198a890",
            "placeholder": "​",
            "style": "IPY_MODEL_2ecef226f25d4564a65dd502e44b217d",
            "value": " 2/2 [00:01&lt;00:00,  2.00it/s]"
          }
        },
        "1b3906efffc442a2ac1344ed646bf7a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29675e91dc38483cb9501914ce9edf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3906efffc442a2ac1344ed646bf7a0",
            "placeholder": "​",
            "style": "IPY_MODEL_8ae0fb820440436995f40fe18bb5ca2c",
            "value": "100%"
          }
        },
        "2ecef226f25d4564a65dd502e44b217d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fa140acc79141e7beb9da64648869be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae0fb820440436995f40fe18bb5ca2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95dcebd0f8924d98a9802b9bb198a890": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d7c56fa50347c0b7abfea9979f4ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4e350cb7bf645d086d2c5156ff974d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca799509779649b68d8ee3d5572a6c70",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4d7c56fa50347c0b7abfea9979f4ea7",
            "value": 2
          }
        },
        "ca799509779649b68d8ee3d5572a6c70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
